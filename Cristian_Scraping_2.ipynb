{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "022c386c-30e9-4cb0-83d2-f519b40698e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links processed: 10\n",
      "Data saved to shared_mobility_data.csv (Links processed: 10)\n",
      "Links processed: 20\n",
      "Data saved to shared_mobility_data.csv (Links processed: 20)\n",
      "Links processed: 30\n",
      "Data saved to shared_mobility_data.csv (Links processed: 30)\n",
      "Links processed: 40\n",
      "Data saved to shared_mobility_data.csv (Links processed: 40)\n",
      "Links processed: 50\n",
      "Data saved to shared_mobility_data.csv (Links processed: 50)\n",
      "Links processed: 60\n",
      "Data saved to shared_mobility_data.csv (Links processed: 60)\n",
      "Links processed: 70\n",
      "Data saved to shared_mobility_data.csv (Links processed: 70)\n",
      "Links processed: 80\n",
      "Data saved to shared_mobility_data.csv (Links processed: 80)\n",
      "Links processed: 90\n",
      "Data saved to shared_mobility_data.csv (Links processed: 90)\n",
      "Links processed: 100\n",
      "Data saved to shared_mobility_data.csv (Links processed: 100)\n",
      "Links processed: 110\n",
      "Data saved to shared_mobility_data.csv (Links processed: 110)\n",
      "Links processed: 120\n",
      "Data saved to shared_mobility_data.csv (Links processed: 120)\n",
      "Links processed: 130\n",
      "Data saved to shared_mobility_data.csv (Links processed: 130)\n",
      "Links processed: 140\n",
      "Data saved to shared_mobility_data.csv (Links processed: 140)\n",
      "Links processed: 144\n",
      "Failed to retrieve the page. Status code: 404\n",
      "Data saved to shared_mobility_data.csv (Links processed: 144)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Newsapi.org\n",
    "--> IF THEY MATCH OR OVERLAP, DROP DUPLICATE\n",
    "        This code could also be used in general and for the news API. \n",
    "        It does not delete the link or source at the end of each mobility news article. \n",
    "\n",
    "Alicja's and Cristian's code. The aim is to scrape data from a Mobility News website and store it in a Data Frame. \n",
    "After this, the next step would be to find an API to analyze the text data stored.\n",
    "\n",
    "This code is faster. Retrieves 200 links in about 1 minute. It removes the link at the end of each article. \n",
    "Give it some time to begin printing.\n",
    "\n",
    "It has no link limitation. Adds data to the DataFrame after each 10 links. \n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "base_url = \"https://www.sharedmobility.news/category/mob/ride/\"\n",
    "page_number = 1\n",
    "\n",
    "#make lists for the data\n",
    "article_texts = []\n",
    "article_urls = []\n",
    "article_titles = []\n",
    "\n",
    "#function to fetch and process articles\n",
    "def process_article(link):\n",
    "    article_response = requests.get(link)\n",
    "    if article_response.status_code == 200:\n",
    "        article_html_content = article_response.text\n",
    "        article_soup = BeautifulSoup(article_html_content, 'html.parser')\n",
    "\n",
    "        # get title and text\n",
    "        article_title = article_soup.find('h1', class_='entry-title').get_text()\n",
    "        article_text = article_soup.find('div', class_='entry-content').get_text()\n",
    "\n",
    "        #add to lists\n",
    "        article_titles.append(article_title)\n",
    "        article_texts.append(article_text)\n",
    "        article_urls.append(link)\n",
    "\n",
    "#counter for processed links\n",
    "link_counter = 0\n",
    "\n",
    "while True:  #collect articles (NO LIMIT)\n",
    "    #construct the page URL\n",
    "    url = f\"{base_url}page/{page_number}/\" if page_number > 1 else base_url\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        #parse HTML\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        #find all the links within the 'a' element within the main section\n",
    "        link_elements = soup.find('main', class_='site-main rbc-content').find_all('a', class_='p-url')\n",
    "\n",
    "        #process articles: multithreading\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_article, link_element.get('href')) for link_element in link_elements]\n",
    "\n",
    "        #update counter\n",
    "        link_counter += len(link_elements)\n",
    "       \n",
    "\n",
    "        #if the link counter is a multiple of 10, update DataFrame\n",
    "        if link_counter % 10 == 0:\n",
    "            #df with extracted data\n",
    "            data = {\n",
    "                \"ID\": range(1, link_counter + 1),  # Use the link counter for the ID\n",
    "                \"Text\": article_texts,\n",
    "                \"URL\": article_urls,\n",
    "                \"Article Title\": article_titles\n",
    "            }\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            #save as CSV file\n",
    "            df.to_csv(\"shared_mobility_data.csv\", index=False)\n",
    "            print(f\"Data saved to shared_mobility_data.csv (Links processed: {link_counter})\")\n",
    "\n",
    "        #if no links are found\n",
    "        if not link_elements:\n",
    "            break\n",
    "\n",
    "        page_number += 1  #go to next page\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "        break\n",
    "\n",
    "#create final DataFrame with all data\n",
    "data = {\n",
    "    \"ID\": range(1, link_counter + 1),  # Use the link counter for the ID\n",
    "    \"Text\": article_texts,\n",
    "    \"URL\": article_urls,\n",
    "    \"Article Title\": article_titles\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#save as CSV file\n",
    "df.to_csv(\"shared_mobility_data.csv\", index=False)\n",
    "print(f\"Data saved to shared_mobility_data.csv (Links processed: {link_counter})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c7496a6-930f-4c10-a73f-21ff7b6c61f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the page. Status code: 404\n",
      "Data saved to shared_mobility_data.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "REMOVES LINK AT THE BOTTOM OF EACH ARTICLE. However, it cannot remove both links. \n",
    "\n",
    "\n",
    "Cannot be used for News API. \n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re  # Import the 're' module\n",
    "\n",
    "# Define a function to clean the article text\n",
    "def clean_article_text(article_text):\n",
    "    # Define a pattern to match the source and link sections\n",
    "    pattern = r'Source: https://\\S+'\n",
    "    \n",
    "    # Use re.sub to remove the matched pattern from the text\n",
    "    cleaned_text = re.sub(pattern, '', article_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "base_url = \"https://www.sharedmobility.news/category/mob/ride/\"\n",
    "page_number = 1\n",
    "\n",
    "# Store the data in lists\n",
    "article_texts = []\n",
    "article_urls = []\n",
    "article_titles = []\n",
    "\n",
    "# Set the limit for the number of links to collect\n",
    "link_limit = 200\n",
    "\n",
    "# Define a function to fetch and process an article\n",
    "def process_article(link):\n",
    "    article_response = requests.get(link)\n",
    "    if article_response.status_code == 200:\n",
    "        article_html_content = article_response.text\n",
    "        article_soup = BeautifulSoup(article_html_content, 'html.parser')\n",
    "\n",
    "        # Extract the article title and text\n",
    "        article_title = article_soup.find('h1', class_='entry-title').get_text()\n",
    "        article_text = article_soup.find('div', class_='entry-content').get_text()\n",
    "\n",
    "        # Clean the article text using the clean_article_text function\n",
    "        cleaned_article_text = clean_article_text(article_text)\n",
    "\n",
    "        # Add data to lists\n",
    "        article_titles.append(article_title)\n",
    "        article_texts.append(cleaned_article_text)\n",
    "        article_urls.append(link)\n",
    "\n",
    "# Initialize a counter for processed links\n",
    "link_counter = 0\n",
    "\n",
    "# While loop to fetch articles until the link limit is reached\n",
    "while link_counter < link_limit:\n",
    "    # Construct the URL for the current page\n",
    "    url = f\"{base_url}page/{page_number}/\" if page_number > 1 else base_url\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Finding all the links within the 'a' element within the main section\n",
    "        link_elements = soup.find('main', class_='site-main rbc-content').find_all('a', class_='p-url')\n",
    "\n",
    "        # Process the articles using multithreading\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_article, link_element.get('href')) for link_element in link_elements]\n",
    "\n",
    "        # Update the link counter\n",
    "        link_counter += len(link_elements)\n",
    "\n",
    "        page_number += 1  # Move to the next page\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "        break\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    \"ID\": range(1, link_counter + 1),  # Use the link counter for the ID\n",
    "    \"Text\": article_texts,\n",
    "    \"URL\": article_urls,\n",
    "    \"Article Title\": article_titles\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(\"shared_mobility_data.csv\", index=False)\n",
    "print(\"Data saved to shared_mobility_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
